{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        X, Y = pickle.load(file)\n",
    "    return X, Y\n",
    "X1, Y1 = load_processed_data('./pickel_files/all_data_200_100.pkl')\n",
    "X1 = np.array(X1)\n",
    "Y1 = np.array(Y1)\n",
    "Y1 = Y1[:, :2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming combined_data is your combined dictionary with 'X' and 'Y' keys\n",
    "combined_data = {'X': [], 'Y': []}\n",
    "combined_data['X'].extend(X1)\n",
    "combined_data['Y'].extend(Y1)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "def process_and_combine_pkl_files(directory_path):\n",
    "    \n",
    "    # Step 1: Find all .pkl files in the specified directory\n",
    "    for file_path in glob.glob(directory_path + '/*.pkl'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        # Step 2: Append the data to combined_data if both 'X' and 'Y' are present\n",
    "        if 'X' in data and 'Y' in data:\n",
    "            combined_data['X'].extend(data['X'])\n",
    "            combined_data['Y'].extend(data['Y'])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Step 4: Convert lists to NumPy arrays\n",
    "    X_filtered = np.array(combined_data['X'])\n",
    "    Y_filtered = np.array(combined_data['Y'])\n",
    "    \n",
    "    # Further processing of Y (if needed)\n",
    "    if Y_filtered.shape[1] > 2:\n",
    "        Y_filtered = Y_filtered[:, :2]\n",
    "\n",
    "    # Convert X to float 16\n",
    "    X_filtered = X_filtered.astype(np.float16)\n",
    "    \n",
    "    # Convert Y to float 16 \n",
    "    Y_filtered = Y_filtered.astype(np.float16)\n",
    "    \n",
    "    return X_filtered, Y_filtered\n",
    "\n",
    "# Example usage\n",
    "directory_path = './process_MPIIGaze'  # Adjust this path as necessary\n",
    "X_filtered, Y_filtered = process_and_combine_pkl_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save X and Y\n",
    "with open('60K', 'wb') as file:\n",
    "    pickle.dump([X_filtered, Y_filtered], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load X and Y\n",
    "with open('60K', 'rb') as file:\n",
    "    X_filtered, Y_filtered = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_generator(X, Y, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    A generator that yields batches of data indefinitely, with optional shuffling.\n",
    "    \"\"\"\n",
    "    num_samples = len(X)\n",
    "    while True:  # Loop indefinitely\n",
    "        if shuffle:\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            yield X[start_idx:end_idx], Y[start_idx:end_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "from keras.regularizers import l2\n",
    "# Maxpooling\n",
    "from keras.layers import MaxPooling2D\n",
    "#BatchNormalization\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Your model definition\n",
    "model = Sequential([\n",
    "    Conv2D(32, (7, 7), activation='relu', input_shape=(100, 200, 3), kernel_regularizer=l2(0.001)),\n",
    "    \n",
    "    \n",
    "    Conv2D(64, (7, 7), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "\n",
    "\n",
    "    Conv2D(128, (5, 5), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.15),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "\n",
    "    Conv2D(256, (5, 5), activation='relu'),\n",
    "\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00005), loss='mse', metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits data into training, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X, Y: The features and labels.\n",
    "    - test_size: Fraction of the dataset to be used as the test set.\n",
    "    - val_size: Fraction of the dataset (after removing the test set) to be used as the validation set.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, Y_train: Training set.\n",
    "    - X_val, Y_val: Validation set.\n",
    "    - X_test, Y_test: Test set.\n",
    "    \"\"\"\n",
    "    # Shuffle the dataset\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    Y = Y[indices]\n",
    "    \n",
    "    # Calculate split indices\n",
    "    test_split_idx = int(len(X) * (1 - test_size))\n",
    "    val_split_idx = int(test_split_idx * (1 - val_size))\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, X_test = X[:val_split_idx], X[val_split_idx:test_split_idx], X[test_split_idx:]\n",
    "    Y_train, Y_val, Y_test = Y[:val_split_idx], Y[val_split_idx:test_split_idx], Y[test_split_idx:]\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "\n",
    "# Apply the function to your data\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = split_data(X_filtered, Y_filtered, test_size=0.15, val_size=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "num_train_samples = int(len(X_filtered) * 0.6)  # Assuming 60% of data is for training\n",
    "num_val_samples = int(len(X_filtered) * 0.2)  # Assuming 20% of data is for validation\n",
    "\n",
    "steps_per_epoch = num_train_samples // batch_size\n",
    "validation_steps = num_val_samples // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(\n",
    "    x=batch_generator(X_train, Y_train, batch_size=batch_size, shuffle=True),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=100,  # Number of epochs to train for\n",
    "    validation_data=batch_generator(X_val, Y_val, batch_size=batch_size, shuffle=False),\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "    model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./models/eye_gaze_v18.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predicted vs actual on test data on a canvas using opencv \n",
    "\n",
    "import cv2\n",
    "import numpy as np \n",
    "predictions = model.predict(X_test[:25])\n",
    "screen_width, screen_height = 2650, 1440\n",
    "canvas = np.zeros((screen_height, screen_width, 3), dtype=np.uint8)\n",
    "cv2.namedWindow('Gaze Tracking on Canvas', cv2.WINDOW_NORMAL)\n",
    "cv2.setWindowProperty('Gaze Tracking on Canvas', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "# plot the first 10 images one by one\n",
    "for i in range(0,25):\n",
    "    \n",
    "    # get the predicted x,y coordinates\n",
    "    x, y = predictions[i][0] * screen_width, predictions[i][1] * screen_height\n",
    "\n",
    "    # lock the preds \n",
    "    x = min(max(x, 0), screen_width)\n",
    "    y = min(max(y, 0), screen_height)\n",
    "\n",
    "    # get the actual x,y coordinates\n",
    "    x_actual, y_actual = Y_test[i][0] * screen_width, Y_test[i][1] * screen_height\n",
    "\n",
    "    # plot the predicted x,y coordinates\n",
    "    cv2.circle(canvas, (int(x), int(y)), 10, (0, 0, 255), -1)\n",
    "    # plot the actual x,y coordinates\n",
    "    cv2.circle(canvas, (int(x_actual), int(y_actual)), 10, (0, 255, 0), -1 )\n",
    "    # show the canvas\n",
    "    cv2.imshow('Gaze Tracking on Canvas', canvas)\n",
    "    cv2.waitKey(0)\n",
    "    # # show the image \n",
    "    # cv2.imshow('image', X_test[i])\n",
    "    # cv2.waitKey(0)\n",
    "    # # clear the canvas\n",
    "    \n",
    "    canvas = np.zeros((screen_height, screen_width, 3), dtype=np.uint8)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/eye_gaze_v21.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.10_eye_gaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
