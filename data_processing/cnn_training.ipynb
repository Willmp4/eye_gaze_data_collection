{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eye_region(frame, eye_coords, target_size=(30, 36)):\n",
    "    \"\"\"\n",
    "    Preprocesses the eye region for the CNN model.\n",
    "    Args:\n",
    "        frame: The input image frame (in BGR format).\n",
    "        eye_coords: Coordinates of the eye region.\n",
    "        target_size: The target size for each eye region.\n",
    "    Returns:\n",
    "        The preprocessed eye region.\n",
    "    \"\"\"\n",
    "    x_min = min(x for x, y in eye_coords)\n",
    "    x_max = max(x for x, y in eye_coords)\n",
    "    y_min = min(y for x, y in eye_coords)\n",
    "    y_max = max(y for x, y in eye_coords)\n",
    "\n",
    "    # Cropping the eye region based on the extremities of the landmarks\n",
    "    cropped_eye = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    # Resizing the cropped eye region to the target size\n",
    "    resized_eye = cv2.resize(cropped_eye, target_size)\n",
    "\n",
    "    return resized_eye.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_eyes(frame):\n",
    "    \"\"\"\n",
    "    Detects and combines the eye regions from the frame.\n",
    "    Args:\n",
    "        frame: The input image frame.\n",
    "    Returns:\n",
    "        The combined eye regions, or None if not detected.\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Extract the coordinates for each eye\n",
    "        left_eye = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)]\n",
    "        right_eye = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)]\n",
    "\n",
    "        # Preprocess each eye region\n",
    "        left_eye_region = preprocess_eye_region(frame, left_eye)\n",
    "\n",
    "        right_eye_region = preprocess_eye_region(frame, right_eye)\n",
    "\n",
    "        # Combine the eyes side by side\n",
    "        combined_eyes = np.hstack([left_eye_region, right_eye_region])\n",
    "\n",
    "        # Ensure the combined eyes image has the correct shape\n",
    "        if combined_eyes.shape[1] != 60:\n",
    "            raise ValueError(\"Combined eyes region does not match the expected width.\")\n",
    "        return combined_eyes\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_head_pose(head_pose_data, rotation_scale=180, translation_max_displacement=None):\n",
    "    \"\"\"\n",
    "    Normalizes the head pose data.\n",
    "    Args:\n",
    "        head_pose_data: List containing the head pose data (rotation and translation vectors).\n",
    "        rotation_scale: Maximum value for the rotation vector components (180 for degrees, np.pi for radians).\n",
    "        translation_max_displacement: A tuple (max_x, max_y, max_z) representing the maximum displacement in each axis. If None, standard deviation normalization will be used.\n",
    "\n",
    "    Returns:\n",
    "        Normalized head pose data.\n",
    "    \"\"\"\n",
    "    # Normalize rotation vectors\n",
    "    normalized_rotation = np.array(head_pose_data[:3]) / rotation_scale\n",
    "\n",
    "    # Normalize translation vectors\n",
    "    if translation_max_displacement:\n",
    "        max_x, max_y, max_z = translation_max_displacement\n",
    "        normalized_translation = np.array(head_pose_data[3:]) / np.array([max_x, max_y, max_z])\n",
    "    else:\n",
    "        # Standard deviation normalization\n",
    "        translation_vector = np.array(head_pose_data[3:])\n",
    "        std_dev = np.std(translation_vector)\n",
    "        mean_val = np.mean(translation_vector)\n",
    "        normalized_translation = (translation_vector - mean_val) / std_dev\n",
    "\n",
    "    return np.concatenate([normalized_rotation, normalized_translation]).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "# Assuming normalize_head_pose and get_combined_eyes are defined as before\n",
    "def get_screen_size(metadata_file_path):\n",
    "    with open(metadata_file_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "        # Check if 'screenData' is a key in the metadata\n",
    "        if 'screenData' in metadata:\n",
    "            metadata = metadata['screenData']\n",
    "        # Otherwise, assume the metadata is already at the top level\n",
    "\n",
    "        screen_width = metadata.get('screenWidth')\n",
    "        screen_height = metadata.get('screenHeight')\n",
    "\n",
    "        if screen_width is None or screen_height is None:\n",
    "            raise ValueError(\"Screen size not found in metadata\")\n",
    "\n",
    "        return screen_width, screen_height\n",
    "\n",
    "def parse_head_pose_data(row):\n",
    "    # Split the strings and convert to float\n",
    "    rotation_str, translation_str = row['head_pose'], row['head_translation']\n",
    "    rotation = [float(x) for x in rotation_str.strip('\"').split(',')]\n",
    "    translation = [float(x) for x in translation_str.strip('\"').split(',')]\n",
    "    return rotation + translation  # Combine into a single list\n",
    "\n",
    "def prepare_dataset(base_dir):\n",
    "    X, Y = [], []\n",
    "    processed_files = set()\n",
    "    column_names = ['image_path', 'cursor_x', 'cursor_y', 'left_pup', 'eye_y1', 'eye_x2', 'eye_y2', 'eye_x3', 'eye_y3', 'eye_x4', 'eye_y4', 'eye_x5', 'eye_y5', 'eye_x6', 'eye_y6', 'head_pose', 'head_translation']\n",
    "\n",
    "    for subdir in glob(os.path.join(base_dir, '*/')):\n",
    "        print(f\"Processing directory: {subdir}\")\n",
    "        metadata_file_path = os.path.join(subdir, 'metadata.json')\n",
    "        screen_width, screen_height = get_screen_size(metadata_file_path)\n",
    "        print(f\"Screen size: {screen_width}x{screen_height}\")\n",
    "\n",
    "        # Find any CSV files in the directory\n",
    "        csv_files = glob(os.path.join(subdir, '*.csv'))\n",
    "        #skip calibration files\n",
    "        csv_files = [f for f in csv_files if 'calibration' not in f]\n",
    "\n",
    "        for data_file_path in csv_files:\n",
    "            if data_file_path in processed_files:\n",
    "                # Skip this file since it has already been processed\n",
    "                continue\n",
    "            processed_files.add(data_file_path)  # Mark this file as processed\n",
    "\n",
    "            print(f\"Processing data CSV file: {data_file_path}\")\n",
    "            data = pd.read_csv(data_file_path, header=None, names=column_names)\n",
    "\n",
    "            if not csv_files:\n",
    "                print(f\"No data CSV file found in directory: {subdir}\")\n",
    "                continue\n",
    "            # Find any directory that contains image files (assuming JPEG for example)\n",
    "            img_folders = [d for d in os.listdir(subdir) if os.path.isdir(os.path.join(subdir, d)) and glob(os.path.join(subdir, d, '*.png'))]\n",
    "            if not img_folders:\n",
    "                print(f\"No image folder found that contains images in directory: {subdir}\")\n",
    "                continue\n",
    "            data = pd.read_csv(data_file_path, header=None, names=column_names)\n",
    "            # print how many columns \n",
    "            print(data.shape)\n",
    "\n",
    "            for index, row in data.iterrows():\n",
    "                # Directly use the image path from the dataframe\n",
    "                img_path = os.path.join(row['image_path'])\n",
    "                cursor_x, cursor_y = row['cursor_x'], row['cursor_y']\n",
    "                eye_box_pupil_data = row[3:15].tolist()\n",
    "                head_pose_data = parse_head_pose_data(row)\n",
    "\n",
    "                normalized_eye_box_pupil_data = [float(coord) / screen_width if i % 2 == 0 else float(coord) / screen_height for i, coord in enumerate(eye_box_pupil_data)]\n",
    "                normalized_head_pose_data = normalize_head_pose(head_pose_data)\n",
    "\n",
    "                # Load the image\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Image not found: {img_path}\")\n",
    "                    continue\n",
    "\n",
    "                combined_eyes = get_combined_eyes(img)\n",
    "\n",
    "                # Append to datasets\n",
    "                Y.append([cursor_x / screen_width, cursor_y / screen_height] + normalized_eye_box_pupil_data + normalized_head_pose_data)\n",
    "                X.append(combined_eyes)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: ./data\\eloise\\\n",
      "Screen size: 1440x900\n",
      "Processing directory: ./data\\Hossein\\\n",
      "Screen size: 1536x864\n",
      "Processing data CSV file: ./data\\Hossein\\data.csv\n",
      "(19, 17)\n",
      "Processing directory: ./data\\koala\\\n",
      "Screen size: 1536x864\n",
      "Processing data CSV file: ./data\\koala\\eye_gaze_data.csv\n",
      "(5, 17)\n",
      "Processing directory: ./data\\melissa\\\n",
      "Screen size: 1710x1112\n",
      "Processing data CSV file: ./data\\melissa\\data.csv\n",
      "(74, 17)\n",
      "Processing directory: ./data\\Naia\\\n",
      "Screen size: 1440x900\n",
      "Processing data CSV file: ./data\\Naia\\eye_gaze_data.csv\n",
      "(100, 17)\n",
      "Processing directory: ./data\\PerfectUser\\\n",
      "Screen size: 1536x864\n",
      "Processing data CSV file: ./data\\PerfectUser\\eye_gaze_data.csv\n",
      "(4, 17)\n",
      "Processing directory: ./data\\Shaq\\\n",
      "Screen size: 1280x720\n",
      "Processing data CSV file: ./data\\Shaq\\data.csv\n",
      "(29, 17)\n",
      "Processing directory: ./data\\William\\\n",
      "Screen size: 1707x960\n",
      "Processing data CSV file: ./data\\William\\eye_gaze_data.csv\n",
      "(480, 17)\n",
      "Processing directory: ./data\\WilliamOld\\\n",
      "Screen size: 2560x1440\n",
      "Processing data CSV file: ./data\\WilliamOld\\data1.csv\n",
      "(1810, 17)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "base_dir = './data'\n",
    "X, Y = prepare_dataset(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_filtered = [img for img in X if img is not None and isinstance(img, np.ndarray)]\n",
    "Y_filtered = [Y[i] for i in range(len(Y)) if X[i] is not None and isinstance(X[i], np.ndarray)]\n",
    "\n",
    "X_filtered = np.array(X_filtered)\n",
    "\n",
    "Y_filtered = np.array(Y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2521, 2521)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_filtered), len(Y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2352, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_filtered = Y_filtered[:, :14]\n",
    "Y_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_filtered, Y_filtered, test_size=0.2, random_state=42)\n",
    "#Val data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(36, 60, 3)), \n",
    "    MaxPool2D(),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPool2D(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(20) \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[MeanSquaredError(), MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Assuming you have your training data in train_data and train_labels\n",
    "train_generator = train_datagen.flow(X_train, Y_train, batch_size=32)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Adding L2 Regularization to Convolutional Layers\n",
    "l2_reg = 0.001\n",
    "\n",
    "# First Conv Block\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(36, 60, 3), kernel_regularizer=l2(l2_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Second Conv Block\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Third Conv Block\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Flatten and Dense Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(14, activation='sigmoid', kernel_regularizer=l2(l2_reg)))  # Adjust the number of outputs as needed\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,  # Adjust number of epochs\n",
    "    validation_data=(X_val, Y_val),  # Assuming validation data is available\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers with dropout\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(36, 60, 3)))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.25))  # Dropout layer after pooling\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.25))  # Another dropout layer\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.4))  # Higher dropout rate for deeper layers\n",
    "\n",
    "# Flatten the output from convolutional layers before passing it to the dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add dense layers with dropout\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer before the output layer\n",
    "model.add(Dense(14, activation='sigmoid'))  # Adjust the number of outputs as needed\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mean_squared_error', 'mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 3s 49ms/step - loss: 0.0425 - mean_squared_error: 0.0425 - mean_absolute_error: 0.1174 - val_loss: 0.0134 - val_mean_squared_error: 0.0134 - val_mean_absolute_error: 0.0610\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 0.0127 - mean_squared_error: 0.0127 - mean_absolute_error: 0.0524 - val_loss: 0.0113 - val_mean_squared_error: 0.0113 - val_mean_absolute_error: 0.0523\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 0.0115 - mean_squared_error: 0.0115 - mean_absolute_error: 0.0514 - val_loss: 0.0102 - val_mean_squared_error: 0.0102 - val_mean_absolute_error: 0.0539\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0492 - val_loss: 0.0085 - val_mean_squared_error: 0.0085 - val_mean_absolute_error: 0.0465\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 0.0095 - mean_squared_error: 0.0095 - mean_absolute_error: 0.0472 - val_loss: 0.0080 - val_mean_squared_error: 0.0080 - val_mean_absolute_error: 0.0422\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 3s 59ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - mean_absolute_error: 0.0427 - val_loss: 0.0070 - val_mean_squared_error: 0.0070 - val_mean_absolute_error: 0.0382\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 2s 52ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - mean_absolute_error: 0.0405 - val_loss: 0.0069 - val_mean_squared_error: 0.0069 - val_mean_absolute_error: 0.0392\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 2s 52ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - mean_absolute_error: 0.0379 - val_loss: 0.0063 - val_mean_squared_error: 0.0063 - val_mean_absolute_error: 0.0360\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 2s 51ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - mean_absolute_error: 0.0369 - val_loss: 0.0059 - val_mean_squared_error: 0.0059 - val_mean_absolute_error: 0.0341\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - mean_absolute_error: 0.0352 - val_loss: 0.0068 - val_mean_squared_error: 0.0068 - val_mean_absolute_error: 0.0383\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - mean_absolute_error: 0.0347 - val_loss: 0.0064 - val_mean_squared_error: 0.0064 - val_mean_absolute_error: 0.0398\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 2s 51ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - mean_absolute_error: 0.0354 - val_loss: 0.0057 - val_mean_squared_error: 0.0057 - val_mean_absolute_error: 0.0336\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - mean_absolute_error: 0.0351 - val_loss: 0.0067 - val_mean_squared_error: 0.0067 - val_mean_absolute_error: 0.0357\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - mean_absolute_error: 0.0338 - val_loss: 0.0056 - val_mean_squared_error: 0.0056 - val_mean_absolute_error: 0.0325\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 2s 50ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - mean_absolute_error: 0.0323 - val_loss: 0.0054 - val_mean_squared_error: 0.0054 - val_mean_absolute_error: 0.0320\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - mean_absolute_error: 0.0302 - val_loss: 0.0058 - val_mean_squared_error: 0.0058 - val_mean_absolute_error: 0.0329\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - mean_absolute_error: 0.0327 - val_loss: 0.0053 - val_mean_squared_error: 0.0053 - val_mean_absolute_error: 0.0317\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - mean_absolute_error: 0.0303 - val_loss: 0.0058 - val_mean_squared_error: 0.0058 - val_mean_absolute_error: 0.0312\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - mean_absolute_error: 0.0315 - val_loss: 0.0053 - val_mean_squared_error: 0.0053 - val_mean_absolute_error: 0.0314\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 2s 51ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - mean_absolute_error: 0.0291 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0307\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 2s 51ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - mean_absolute_error: 0.0294 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0312\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 2s 50ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - mean_absolute_error: 0.0298 - val_loss: 0.0055 - val_mean_squared_error: 0.0055 - val_mean_absolute_error: 0.0347\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 2s 50ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - mean_absolute_error: 0.0303 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0298\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - mean_absolute_error: 0.0289 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0325\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - mean_absolute_error: 0.0296 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0292\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - mean_absolute_error: 0.0292 - val_loss: 0.0051 - val_mean_squared_error: 0.0051 - val_mean_absolute_error: 0.0303\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 3s 57ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - mean_absolute_error: 0.0287 - val_loss: 0.0059 - val_mean_squared_error: 0.0059 - val_mean_absolute_error: 0.0367\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - mean_absolute_error: 0.0298 - val_loss: 0.0048 - val_mean_squared_error: 0.0048 - val_mean_absolute_error: 0.0295\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - mean_absolute_error: 0.0287 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0297\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - mean_absolute_error: 0.0278 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0296\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 2s 50ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - mean_absolute_error: 0.0270 - val_loss: 0.0051 - val_mean_squared_error: 0.0051 - val_mean_absolute_error: 0.0302\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - mean_absolute_error: 0.0273 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0298\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 2s 49ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - mean_absolute_error: 0.0277 - val_loss: 0.0047 - val_mean_squared_error: 0.0047 - val_mean_absolute_error: 0.0286\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 2s 52ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - mean_absolute_error: 0.0262 - val_loss: 0.0053 - val_mean_squared_error: 0.0053 - val_mean_absolute_error: 0.0307\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - mean_absolute_error: 0.0280 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0294\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - mean_absolute_error: 0.0268 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0296\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - mean_absolute_error: 0.0269 - val_loss: 0.0048 - val_mean_squared_error: 0.0048 - val_mean_absolute_error: 0.0290\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - mean_absolute_error: 0.0261 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0300\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - mean_absolute_error: 0.0268 - val_loss: 0.0054 - val_mean_squared_error: 0.0054 - val_mean_absolute_error: 0.0318\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 2s 46ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - mean_absolute_error: 0.0273 - val_loss: 0.0047 - val_mean_squared_error: 0.0047 - val_mean_absolute_error: 0.0272\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - mean_absolute_error: 0.0254 - val_loss: 0.0048 - val_mean_squared_error: 0.0048 - val_mean_absolute_error: 0.0269\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - mean_absolute_error: 0.0255 - val_loss: 0.0047 - val_mean_squared_error: 0.0047 - val_mean_absolute_error: 0.0275\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - mean_absolute_error: 0.0247 - val_loss: 0.0051 - val_mean_squared_error: 0.0051 - val_mean_absolute_error: 0.0286\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 2s 48ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - mean_absolute_error: 0.0257 - val_loss: 0.0048 - val_mean_squared_error: 0.0048 - val_mean_absolute_error: 0.0283\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 2s 45ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - mean_absolute_error: 0.0247 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0301\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - mean_absolute_error: 0.0247 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0279\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - mean_absolute_error: 0.0247 - val_loss: 0.0051 - val_mean_squared_error: 0.0051 - val_mean_absolute_error: 0.0280\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - mean_absolute_error: 0.0257 - val_loss: 0.0053 - val_mean_squared_error: 0.0053 - val_mean_absolute_error: 0.0314\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 2s 47ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - mean_absolute_error: 0.0250 - val_loss: 0.0051 - val_mean_squared_error: 0.0051 - val_mean_absolute_error: 0.0284\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 2s 50ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - mean_absolute_error: 0.0240 - val_loss: 0.0051 - val_mean_squared_error: 0.0051 - val_mean_absolute_error: 0.0281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x140722e93c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=50, validation_split=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - mean_absolute_error: 0.0284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.008516031317412853, 0.008516031317412853, 0.028364835307002068]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate the model\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "#plot predicted vs actual on test data on a canvas using opencv \n",
    "\n",
    "import cv2\n",
    "import numpy as np \n",
    "predictions = model.predict(X_test)\n",
    "screen_width, screen_height = 2650, 1440\n",
    "canvas = np.zeros((screen_height, screen_width, 3), dtype=np.uint8)\n",
    "cv2.namedWindow('Gaze Tracking on Canvas', cv2.WINDOW_NORMAL)\n",
    "cv2.setWindowProperty('Gaze Tracking on Canvas', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "# plot the first 10 images one by one\n",
    "for i in range(40,50):\n",
    "    # get the predicted x,y coordinates\n",
    "    x, y = predictions[i][0] * screen_width, predictions[i][1] * screen_height\n",
    "    # get the actual x,y coordinates\n",
    "    x_actual, y_actual = Y_test[i][0] * screen_width, Y_test[i][1] * screen_height\n",
    "    # plot the predicted x,y coordinates\n",
    "    cv2.circle(canvas, (int(x), int(y)), 10, (0, 0, 255), -1)\n",
    "    # plot the actual x,y coordinates\n",
    "    cv2.circle(canvas, (int(x_actual), int(y_actual)), 10, (0, 255, 0), -1)\n",
    "    # show the canvas\n",
    "    cv2.imshow('Gaze Tracking on Canvas', canvas)\n",
    "    cv2.waitKey(0)\n",
    "    # clear the canvas\n",
    "    canvas = np.zeros((screen_height, screen_width, 3), dtype=np.uint8)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/eye_gaze_v10_2600.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.10_eye_gaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
