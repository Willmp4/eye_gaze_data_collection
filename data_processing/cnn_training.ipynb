{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        X, Y = pickle.load(file)\n",
    "    return X, Y\n",
    "X1, Y1 = load_processed_data('./pickel_files/all_data_200_100.pkl')\n",
    "X1 = np.array(X1)\n",
    "Y1 = np.array(Y1)\n",
    "Y1 = Y1[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming combined_data is your combined dictionary with 'X' and 'Y' keys\n",
    "combined_data = {'X': [], 'Y': []}\n",
    "combined_data['X'].extend(X1)\n",
    "combined_data['Y'].extend(Y1)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "def process_and_combine_pkl_files(directory_path):\n",
    "    \n",
    "    # Step 1: Find all .pkl files in the specified directory\n",
    "    for file_path in glob.glob(directory_path + '/*.pkl'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        # Step 2: Append the data to combined_data if both 'X' and 'Y' are present\n",
    "        if 'X' in data and 'Y' in data:\n",
    "            combined_data['X'].extend(data['X'])\n",
    "            combined_data['Y'].extend(data['Y'])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Step 4: Convert lists to NumPy arrays\n",
    "    X_filtered = np.array(combined_data['X'])\n",
    "    Y_filtered = np.array(combined_data['Y'])\n",
    "    \n",
    "    # Further processing of Y (if needed)\n",
    "    if Y_filtered.shape[1] > 2:\n",
    "        Y_filtered = Y_filtered[:, :2]\n",
    "\n",
    "    # Convert X to float 16\n",
    "    X_filtered = X_filtered.astype(np.float16)\n",
    "    \n",
    "    # Convert Y to float 16 \n",
    "    Y_filtered = Y_filtered.astype(np.float16)\n",
    "    \n",
    "    return X_filtered, Y_filtered\n",
    "\n",
    "# Example usage\n",
    "directory_path = './process_MPIIGaze'  # Adjust this path as necessary\n",
    "X_filtered, Y_filtered = process_and_combine_pkl_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save X and Y\n",
    "with open('60K', 'wb') as file:\n",
    "    pickle.dump([X_filtered, Y_filtered], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load X and Y\n",
    "with open('60K', 'rb') as file:\n",
    "    X_filtered, Y_filtered = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_generator(X, Y, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    A generator that yields batches of data indefinitely, with optional shuffling.\n",
    "    \"\"\"\n",
    "    num_samples = len(X)\n",
    "    while True:  # Loop indefinitely\n",
    "        if shuffle:\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            yield X[start_idx:end_idx], Y[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "from keras.regularizers import l2\n",
    "# Maxpooling\n",
    "from keras.layers import MaxPooling2D\n",
    "#BatchNormalization\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Your model definition\n",
    "model = Sequential([\n",
    "    Conv2D(32, (7, 7), activation='relu', input_shape=(100, 200, 3), kernel_regularizer=l2(0.001)),\n",
    "    \n",
    "    \n",
    "    Conv2D(64, (7, 7), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "\n",
    "\n",
    "    Conv2D(128, (5, 5), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.15),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "\n",
    "    Conv2D(256, (5, 5), activation='relu'),\n",
    "\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00005), loss='mse', metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits data into training, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X, Y: The features and labels.\n",
    "    - test_size: Fraction of the dataset to be used as the test set.\n",
    "    - val_size: Fraction of the dataset (after removing the test set) to be used as the validation set.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, Y_train: Training set.\n",
    "    - X_val, Y_val: Validation set.\n",
    "    - X_test, Y_test: Test set.\n",
    "    \"\"\"\n",
    "    # Shuffle the dataset\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    Y = Y[indices]\n",
    "    \n",
    "    # Calculate split indices\n",
    "    test_split_idx = int(len(X) * (1 - test_size))\n",
    "    val_split_idx = int(test_split_idx * (1 - val_size))\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, X_test = X[:val_split_idx], X[val_split_idx:test_split_idx], X[test_split_idx:]\n",
    "    Y_train, Y_val, Y_test = Y[:val_split_idx], Y[val_split_idx:test_split_idx], Y[test_split_idx:]\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "\n",
    "# Apply the function to your data\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = split_data(X_filtered, Y_filtered, test_size=0.15, val_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "num_train_samples = int(len(X_filtered) * 0.6)  # Assuming 60% of data is for training\n",
    "num_val_samples = int(len(X_filtered) * 0.2)  # Assuming 20% of data is for validation\n",
    "\n",
    "steps_per_epoch = num_train_samples // batch_size\n",
    "validation_steps = num_val_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(\n",
    "    x=batch_generator(X_train, Y_train, batch_size=batch_size, shuffle=True),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=100,  # Number of epochs to train for\n",
    "    validation_data=batch_generator(X_val, Y_val, batch_size=batch_size, shuffle=False),\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./models/eye_gaze_v21v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pickle file\n",
    "import pickle\n",
    "with open('./pickel_files/all_data_200_100.pkl', 'rb') as file:\n",
    "    X_filtered, Y_filtered = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3978, 100, 200, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_filtered = np.array(X_filtered)\n",
    "Y_filtered = np.array(Y_filtered)\n",
    "\n",
    "Y_filtered = Y_filtered[:, :2]\n",
    "\n",
    "X_filtered.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, Y_train, X_test, Y_test = train_test_split(X_filtered, Y_filtered, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3381, 100, 200, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 100, 200, 3), found shape=(None, 21)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m----> 5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m screen_width, screen_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2650\u001b[39m, \u001b[38;5;241m1440\u001b[39m\n\u001b[0;32m      7\u001b[0m canvas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((screen_height, screen_width, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file4pp3fehd.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\wgoud\\.conda\\envs\\py3_9_tf_12\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 100, 200, 3), found shape=(None, 21)\n"
     ]
    }
   ],
   "source": [
    "#plot predicted vs actual on test data on a canvas using opencv \n",
    "\n",
    "import cv2\n",
    "import numpy as np \n",
    "predictions = model.predict(X_test[:25])\n",
    "screen_width, screen_height = 2650, 1440\n",
    "canvas = np.zeros((screen_height, screen_width, 3), dtype=np.uint8)\n",
    "cv2.namedWindow('Gaze Tracking on Canvas', cv2.WINDOW_NORMAL)\n",
    "cv2.setWindowProperty('Gaze Tracking on Canvas', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "# plot the first 10 images one by one\n",
    "for i in range(0,25):\n",
    "    \n",
    "    # get the predicted x,y coordinates\n",
    "    x, y = predictions[i][0] * screen_width, predictions[i][1] * screen_height\n",
    "\n",
    "    # lock the preds \n",
    "    x = min(max(x, 0), screen_width)\n",
    "    y = min(max(y, 0), screen_height)\n",
    "\n",
    "    # get the actual x,y coordinates\n",
    "    x_actual, y_actual = Y_test[i][0] * screen_width, Y_test[i][1] * screen_height\n",
    "\n",
    "    # plot the predicted x,y coordinates\n",
    "    cv2.circle(canvas, (int(x), int(y)), 10, (0, 0, 255), -1)\n",
    "    # plot the actual x,y coordinates\n",
    "    cv2.circle(canvas, (int(x_actual), int(y_actual)), 10, (0, 255, 0), -1 )\n",
    "    # show the canvas\n",
    "    cv2.imshow('Gaze Tracking on Canvas', canvas)\n",
    "    cv2.waitKey(0)\n",
    "    # # show the image \n",
    "    # cv2.imshow('image', X_test[i])\n",
    "    # cv2.waitKey(0)\n",
    "    # # clear the canvas\n",
    "    \n",
    "    canvas = np.zeros((screen_height, screen_width, 3), dtype=np.uint8)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/eye_gaze_v21.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.10_eye_gaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
