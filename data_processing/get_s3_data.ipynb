{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "#import image processing from backend\n",
    "from image_processing import * \n",
    "\n",
    "import dlib\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def get_metadata(bucket_name, metadata_file_key, s3_client):\n",
    "    try:\n",
    "        metadata_object = s3_client.get_object(Bucket=bucket_name, Key=metadata_file_key)\n",
    "        metadata_content = metadata_object['Body'].read().decode('utf-8')\n",
    "        metadata = json.loads(metadata_content)\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving metadata from S3: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_info(camera_info):\n",
    "    camera_matrix = np.array(camera_info[0], dtype='double')\n",
    "    dist_coeffs = np.array(camera_info[1], dtype='double')\n",
    "    return camera_matrix, dist_coeffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    dlib_faces = detector(gray)\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for dlib_face in dlib_faces:\n",
    "        shape = predictor(gray, dlib_face)\n",
    "\n",
    "        for (i, (start, end)) in enumerate([(36,42), (42,48)]):\n",
    "            eye_image, (eye_min_x, eye_min_y, eye_max_x, eye_max_y) = extract_eye_region(gray, shape, range(start, end))\n",
    "            eye_image_b = convert_eye_to_binary(eye_image)\n",
    "            pupil_center, _ = detect_pupil(eye_image_b)\n",
    "\n",
    "            if pupil_center:\n",
    "                pupil_center_global = (pupil_center[0] + eye_min_x, pupil_center[1] + eye_min_y)\n",
    "                pupil_center_global = tuple(pc.item() if isinstance(pc, np.generic) else pc for pc in pupil_center_global)\n",
    "                bounding_box = (eye_min_x, eye_min_y, eye_max_x - eye_min_x, eye_max_y - eye_min_y)\n",
    "                bounding_box = tuple(bb.item() if isinstance(bb, np.generic) else bb for bb in bounding_box)\n",
    "\n",
    "\n",
    "                eye_data = {\n",
    "                    'eye_position': 'left' if i == 0 else 'right',\n",
    "                    'pupil_center': pupil_center_global,\n",
    "                    'bounding_box': bounding_box\n",
    "                }\n",
    "                processed_data.append(eye_data)\n",
    "\n",
    "                left_eye_info = None\n",
    "                right_eye_info = None\n",
    "                left_eye_bbox = None\n",
    "                right_eye_bbox = None\n",
    "\n",
    "                for eye_data in processed_data:\n",
    "                    if eye_data['eye_position'] == 'left':\n",
    "                        left_eye_info = eye_data['pupil_center']\n",
    "                        left_eye_bbox = eye_data['bounding_box']\n",
    "                    else:\n",
    "                        right_eye_info = eye_data['pupil_center']\n",
    "                        right_eye_bbox = eye_data['bounding_box']\n",
    "        break\n",
    "    return processed_data, left_eye_info, right_eye_info, left_eye_bbox, right_eye_bbox, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_calibration_data_row(calibratoin_points, left_eye_info, right_eye_info, left_eye_bbox, right_eye_bbox, head_pose):\n",
    "    rotation_vector, translation_vector = head_pose if head_pose else (np.zeros((3, 1)), np.zeros((3, 1)))\n",
    "\n",
    "    rotation_vector_str = ','.join(map(str, rotation_vector.flatten()))\n",
    "    translation_vector_str = ','.join(map(str, translation_vector.flatten()))\n",
    "\n",
    "    data_row = [\n",
    "        calibratoin_points[0], calibratoin_points[1],\n",
    "        left_eye_info[0], left_eye_info[1],\n",
    "        *left_eye_bbox,\n",
    "        right_eye_info[0], right_eye_info[1],\n",
    "        *right_eye_bbox,\n",
    "        rotation_vector_str, translation_vector_str\n",
    "    ]\n",
    "    return data_row\n",
    "\n",
    "def format_eye_gaze_data_row(cursors_position, left_eye_info, right_eye_info, left_eye_bbox, right_eye_bbox, head_pose):\n",
    "    rotation_vector, translation_vector = head_pose if head_pose else (np.zeros((3, 1)), np.zeros((3, 1)))\n",
    "\n",
    "    rotation_vector_str = ','.join(map(str, rotation_vector.flatten()))\n",
    "    translation_vector_str = ','.join(map(str, translation_vector.flatten()))\n",
    "\n",
    "    data_row = [\n",
    "        cursors_position[0], cursors_position[1],\n",
    "        left_eye_info[0], left_eye_info[1],\n",
    "        *left_eye_bbox,\n",
    "        right_eye_info[0], right_eye_info[1],\n",
    "        *right_eye_bbox,\n",
    "        rotation_vector_str, translation_vector_str\n",
    "    ]\n",
    "    return data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_replace_csv(local_base_dir, subdirectory, csv_file_name, image_data):\n",
    "    # Path for the CSV file within the subdirectory\n",
    "    csv_dir_path = os.path.join(local_base_dir, subdirectory)\n",
    "    os.makedirs(csv_dir_path, exist_ok=True)  # Ensure the subdirectory exists\n",
    "    csv_path = os.path.join(csv_dir_path, csv_file_name)\n",
    "    \n",
    "    # Create a new DataFrame for the CSV data\n",
    "    new_data_df = pd.DataFrame(image_data)\n",
    "    \n",
    "    # Write the new DataFrame to the CSV file, overwriting the old data\n",
    "    new_data_df.to_csv(csv_path, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(bucket_name, key_prefix, subdirectory, s3_client):\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    image_paths = []\n",
    "\n",
    "    # List objects within a specific subdirectory\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=f\"{key_prefix}{subdirectory}\"):\n",
    "        for obj in page.get('Contents', []):\n",
    "            # Skip directories\n",
    "            if obj['Key'].endswith('/'):\n",
    "                continue\n",
    "            image_paths.append(obj['Key'])\n",
    "\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_images(image_paths, local_base_dir, subdirectory, csv_file_name, camera_info):\n",
    "    # This will hold all new rows for the CSV file\n",
    "    image_data = []\n",
    "    \n",
    "    # Path to the current CSV file\n",
    "    current_csv_path = os.path.join(local_base_dir, subdirectory, csv_file_name)\n",
    "    \n",
    "    # Check if the CSV file already exists and read the necessary columns\n",
    "    if os.path.exists(current_csv_path):\n",
    "        current_data_df = pd.read_csv(current_csv_path, header=None)\n",
    "\n",
    "        existing_data = current_data_df.iloc[:, 1:3].values\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    for index, image_path in enumerate(image_paths):\n",
    "        # Assuming the image_path is just the file name, construct the full path\n",
    "        full_image_path = os.path.join(local_base_dir, image_path)\n",
    "        image = cv2.imread(full_image_path)\n",
    "        \n",
    "        # Process the image\n",
    "        processed_data, left_eye_info, right_eye_info, left_eye_bbox, right_eye_bbox, shape = pre_process_image(image)\n",
    "        \n",
    "        # Get head pose data\n",
    "        camera_matrix, dist_coeffs = camera_info\n",
    "        head_pose = get_head_pose(shape, camera_matrix, dist_coeffs)\n",
    "        \n",
    "        if index < len(existing_data):\n",
    "            cursor_or_calibration = existing_data[index]\n",
    "        else:\n",
    "            cursor_or_calibration = [np.nan, np.nan]  # or some default value\n",
    "        \n",
    "        # Format the data row for CSV, including the existing data\n",
    "        if 'calibration' in image_path:\n",
    "            data_row = format_calibration_data_row(cursor_or_calibration, left_eye_info, right_eye_info, left_eye_bbox, right_eye_bbox, head_pose)\n",
    "        else:\n",
    "            data_row = format_eye_gaze_data_row(cursor_or_calibration, left_eye_info, right_eye_info, left_eye_bbox, right_eye_bbox, head_pose)\n",
    "        \n",
    "        # Append to the list of image data\n",
    "        image_data.append([image_path] + data_row)\n",
    "\n",
    "    # Create and replace the CSV file with the new data\n",
    "    create_and_replace_csv(local_base_dir, subdirectory, csv_file_name, image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_metadata(bucket_name, metadata_file_key, s3_client):\n",
    "    try:\n",
    "        metadata_object = s3_client.get_object(Bucket=bucket_name, Key=metadata_file_key)\n",
    "        metadata_content = metadata_object['Body'].read().decode('utf-8')\n",
    "        metadata = json.loads(metadata_content)\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving metadata from S3: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_data(key_prefix, local_base_dir, s3_client, bucket_name):\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=key_prefix):\n",
    "        print(f\"Downloading data from {key_prefix}\")\n",
    "        for obj in page.get('Contents', []):\n",
    "            s3_object_key = obj['Key']\n",
    "            if s3_object_key.endswith('/'):\n",
    "                continue\n",
    "            local_file_path = os.path.join(local_base_dir, s3_object_key)\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                local_file_info = os.stat(local_file_path)\n",
    "                s3_object_info = s3_client.head_object(Bucket=bucket_name, Key=s3_object_key)\n",
    "                if local_file_info.st_size == s3_object_info['ContentLength']:\n",
    "                    print(f\"File {s3_object_key} already exists locally and is up to date. Skipping download.\")\n",
    "                    continue\n",
    "            except (FileNotFoundError, ClientError):\n",
    "                pass\n",
    "\n",
    "            s3_client.download_file(bucket_name, s3_object_key, local_file_path)\n",
    "\n",
    "def process_data_if_needed(key_prefix, local_base_dir, s3_client, bucket_name):\n",
    "    # Retrieve metadata and determine if processing is needed\n",
    "    metadata_key = f\"{key_prefix}metadata.json\"\n",
    "    metadata_object = s3_client.get_object(Bucket=bucket_name, Key=metadata_key)\n",
    "    metadata_content = metadata_object['Body'].read().decode('utf-8')\n",
    "    metadata = json.loads(metadata_content)\n",
    "\n",
    "    # If cameraInfo is present in metadata, process the data\n",
    "    if 'cameraInfo' in metadata:\n",
    "        print(f\"Processing data in {key_prefix}\")\n",
    "        camera_matrix, dist_coeffs = get_camera_info(metadata['cameraInfo'])\n",
    "        \n",
    "        # Get the list of image paths that need processing\n",
    "        calibration_image_paths = get_image_paths(bucket_name, key_prefix, 'calibration_images/', s3_client)\n",
    "        eye_gaze_image_paths = get_image_paths(bucket_name, key_prefix, 'eye_gaze_images/', s3_client)\n",
    "        \n",
    "        subdir_prefix = key_prefix.rstrip('/')  # Ensure the prefix doesn't end with a '/'\n",
    "        \n",
    "        # Process calibration images\n",
    "        if calibration_image_paths:\n",
    "            process_images(calibration_image_paths, local_base_dir, subdir_prefix, 'calibration_data.csv', camera_info=(camera_matrix, dist_coeffs))\n",
    "        \n",
    "        # Process eye gaze images\n",
    "        if eye_gaze_image_paths:\n",
    "            process_images(eye_gaze_image_paths, local_base_dir, subdir_prefix, 'eye_gaze_data.csv', camera_info=(camera_matrix, dist_coeffs))\n",
    "        \n",
    "    else:\n",
    "        print(f\"No processing needed for {key_prefix}\")\n",
    "\n",
    "\n",
    "def process_s3_bucket_data(bucket_name, local_base_dir):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # First, get a list of all metadata files\n",
    "    metadata_keys = get_all_metadata_keys(bucket_name, s3_client)\n",
    "    \n",
    "    # Now process each metadata and its associated directory\n",
    "    for metadata_key in metadata_keys:\n",
    "        subdir_prefix = '/'.join(metadata_key.split('/')[:-1]) + '/'\n",
    "        print(f\"Processing data in {subdir_prefix}\")\n",
    "        download_data(subdir_prefix, local_base_dir, s3_client, bucket_name)\n",
    "        process_data_if_needed(subdir_prefix, local_base_dir, s3_client, bucket_name)\n",
    "\n",
    "def get_all_metadata_keys(bucket_name, s3_client):\n",
    "    metadata_keys = []\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    print(f\"Looking for metadata files in bucket {bucket_name}\")\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix='data/'):\n",
    "        for content in page.get('Contents', []):\n",
    "            key = content['Key']\n",
    "            if key.endswith('metadata.json'):\n",
    "                metadata_keys.append(key)\n",
    "    print(f\"Found {len(metadata_keys)} metadata files\")\n",
    "    return metadata_keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for metadata files in bucket eye-gaze-data\n",
      "Found 15 metadata files\n",
      "Processing data in data/Hossein/\n",
      "Downloading data from data/Hossein/\n",
      "No processing needed for data/Hossein/\n",
      "Processing data in data/Shaq/\n",
      "Downloading data from data/Shaq/\n",
      "No processing needed for data/Shaq/\n",
      "Processing data in data/WILLIAM/\n",
      "Downloading data from data/WILLIAM/\n",
      "Processing data in data/WILLIAM/\n",
      "Processing data in data/WILLY/\n",
      "Downloading data from data/WILLY/\n",
      "Processing data in data/WILLY/\n",
      "Processing data in data/Will/\n",
      "Downloading data from data/Will/\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'eye-gaze-data'\n",
    "local_base_dir = './'\n",
    "\n",
    "os.makedirs(local_base_dir, exist_ok=True)\n",
    "\n",
    "process_s3_bucket_data(bucket_name, local_base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data_processing)",
   "language": "python",
   "name": "data_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
