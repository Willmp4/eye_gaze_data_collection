{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 217636\n",
    "x_shape = (100, 200, 3)\n",
    "y_shape = (2,)\n",
    "\n",
    "x_dtype = 'float32'  # Determine the appropriate dtype\n",
    "y_dtype = 'float32'  # Determine the appropriate dtype\n",
    "\n",
    "# x_memmap = np.memmap('x_dataset.memmap', dtype=x_dtype, mode='w+', shape=(num_samples,) + x_shape)\n",
    "# y_memmap = np.memmap('y_dataset.memmap', dtype=y_dtype, mode='w+', shape=(num_samples,) + y_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load x_memmap and y_memmap\n",
    "x_memmap = np.memmap('x_dataset.memmap', dtype=x_dtype, mode='r+', shape=(num_samples,) + x_shape)\n",
    "y_memmap = np.memmap('y_dataset.memmap', dtype=y_dtype, mode='r+', shape=(num_samples,) + y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def process_and_combine_pkl_files_to_memmap(directory_path, x_memmap, y_memmap):\n",
    "    current_index = 0\n",
    "    \n",
    "    for file_path in glob.glob(directory_path + '/*.pkl'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            \n",
    "        # Check if data is a dictionary with 'X' and 'Y' keys\n",
    "        if isinstance(data, dict) and 'X' in data and 'Y' in data:\n",
    "            X_data = data['X']\n",
    "            Y_data = data['Y']\n",
    "        else:\n",
    "            # Assume that the data is just the raw arrays for X and Y\n",
    "            Y_numeric = np.array([y[:2] for y in data[1]], dtype=np.float32)  # Convert the first two elements to float\n",
    "            X_data = np.array(data[0], dtype=np.float32)\n",
    "            Y_data = Y_numeric\n",
    "\n",
    "        num_samples_in_file = len(X_data)\n",
    "        x_batch = np.array(X_data, dtype=x_memmap.dtype).reshape((num_samples_in_file,) + x_shape)\n",
    "        y_batch = np.array(Y_data, dtype=y_memmap.dtype).reshape((num_samples_in_file,) + y_shape)\n",
    "        \n",
    "        # Ensure we do not exceed the allocated memmap size\n",
    "        if current_index + num_samples_in_file > len(x_memmap):\n",
    "            raise ValueError(\"The dataset is larger than expected.\")\n",
    "        \n",
    "        # Write directly to the memmap files\n",
    "        x_memmap[current_index:current_index + num_samples_in_file] = x_batch\n",
    "        y_memmap[current_index:current_index + num_samples_in_file] = y_batch\n",
    "        \n",
    "        current_index += num_samples_in_file\n",
    "        x_memmap.flush()\n",
    "        y_memmap.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './process_MPIIGaze/batches/' \n",
    "process_and_combine_pkl_files_to_memmap(directory_path, x_memmap, y_memmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memmap_batch_generator(x_memmap_path, y_memmap_path, batch_size, shuffle=True):\n",
    "    x_memmap = np.memmap(x_memmap_path, dtype=x_dtype, mode='r', shape=(num_samples,) + x_shape)\n",
    "    y_memmap = np.memmap(y_memmap_path, dtype=y_dtype, mode='r', shape=(num_samples,) + y_shape)\n",
    "    \n",
    "    indices = np.arange(len(x_memmap))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    while True:\n",
    "        for start_idx in range(0, len(indices), batch_size):\n",
    "            end_idx = min(start_idx + batch_size, len(indices))\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            # Ensure batch_indices is an array of integers\n",
    "            batch_indices = np.array(batch_indices, dtype=np.int32)\n",
    "            \n",
    "            # Yield a batch of data\n",
    "            yield x_memmap[batch_indices], y_memmap[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to your memmap files\n",
    "x_memmap_path = 'x_dataset.memmap'\n",
    "y_memmap_path = 'y_dataset.memmap'\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "num_train_samples = int(num_samples * 0.7)  \n",
    "num_val_samples = int(num_samples * 0.15)   \n",
    "\n",
    "steps_per_epoch = num_train_samples // batch_size\n",
    "validation_steps = num_val_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(100, 200, 3))\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(32, (7, 7), activation='relu', kernel_regularizer=l2(0.001))(input_layer)\n",
    "x = Conv2D(64, (7, 7), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(128, (5, 5), activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.15)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(256, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(512, (3, 3), activation='relu')(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(1024, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.175)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Output layer: Adjusted to output 8 values (2 for gaze, 6 for head pose)\n",
    "output_layer = Dense(8, activation='sigmoid')(x)\n",
    "\n",
    "# Model definition\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00005), loss='mse', metrics=['mean_squared_error', 'mean_absolute_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Your model definition\n",
    "model = Sequential([\n",
    "    Conv2D(32, (7, 7), activation='relu', input_shape=(100, 200, 3), kernel_regularizer=l2(0.001)),\n",
    "    \n",
    "    \n",
    "    Conv2D(64, (7, 7), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    " \n",
    "\n",
    "    Conv2D(128, (5, 5), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.15),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(512, (3, 3), activation='relu'),\n",
    "    # MaxPooling2D((2, 2)),\n",
    "    Dropout(0.15),\n",
    "\n",
    "    Conv2D(1024, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.175),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer= Adam(learning_rate=0.00005), loss='mse', metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback to save every 5 epochs\n",
    "checkpoint = ModelCheckpoint('eye_gaze_v23_2_{epoch:02d}.h5', save_freq=5*steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4760/4760 [==============================] - 897s 186ms/step - loss: 0.0507 - mean_squared_error: 0.0256 - mean_absolute_error: 0.1182 - val_loss: 0.0193 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0758\n",
      "Epoch 2/100\n",
      "4760/4760 [==============================] - 706s 148ms/step - loss: 0.0191 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0853 - val_loss: 0.0105 - val_mean_squared_error: 0.0064 - val_mean_absolute_error: 0.0542\n",
      "Epoch 3/100\n",
      "4760/4760 [==============================] - 704s 148ms/step - loss: 0.0149 - mean_squared_error: 0.0113 - mean_absolute_error: 0.0768 - val_loss: 0.0087 - val_mean_squared_error: 0.0055 - val_mean_absolute_error: 0.0526\n",
      "Epoch 4/100\n",
      "4760/4760 [==============================] - 704s 148ms/step - loss: 0.0129 - mean_squared_error: 0.0101 - mean_absolute_error: 0.0725 - val_loss: 0.0081 - val_mean_squared_error: 0.0055 - val_mean_absolute_error: 0.0552\n",
      "Epoch 5/100\n",
      "4760/4760 [==============================] - 705s 148ms/step - loss: 0.0116 - mean_squared_error: 0.0092 - mean_absolute_error: 0.0691 - val_loss: 0.0102 - val_mean_squared_error: 0.0080 - val_mean_absolute_error: 0.0618\n",
      "Epoch 6/100\n",
      "4760/4760 [==============================] - 706s 148ms/step - loss: 0.0105 - mean_squared_error: 0.0085 - mean_absolute_error: 0.0666 - val_loss: 0.0114 - val_mean_squared_error: 0.0096 - val_mean_absolute_error: 0.0691\n",
      "Epoch 7/100\n",
      "4760/4760 [==============================] - 702s 147ms/step - loss: 0.0097 - mean_squared_error: 0.0080 - mean_absolute_error: 0.0643 - val_loss: 0.0067 - val_mean_squared_error: 0.0051 - val_mean_absolute_error: 0.0483\n",
      "Epoch 8/100\n",
      "4760/4760 [==============================] - 703s 148ms/step - loss: 0.0090 - mean_squared_error: 0.0076 - mean_absolute_error: 0.0626 - val_loss: 0.0047 - val_mean_squared_error: 0.0034 - val_mean_absolute_error: 0.0416\n",
      "Epoch 9/100\n",
      "4760/4760 [==============================] - 704s 148ms/step - loss: 0.0084 - mean_squared_error: 0.0072 - mean_absolute_error: 0.0612 - val_loss: 0.0051 - val_mean_squared_error: 0.0039 - val_mean_absolute_error: 0.0401\n",
      "Epoch 10/100\n",
      "4760/4760 [==============================] - 704s 148ms/step - loss: 0.0080 - mean_squared_error: 0.0069 - mean_absolute_error: 0.0600 - val_loss: 0.0049 - val_mean_squared_error: 0.0039 - val_mean_absolute_error: 0.0462\n",
      "Epoch 11/100\n",
      "4760/4760 [==============================] - 706s 148ms/step - loss: 0.0077 - mean_squared_error: 0.0067 - mean_absolute_error: 0.0591 - val_loss: 0.0046 - val_mean_squared_error: 0.0037 - val_mean_absolute_error: 0.0449\n",
      "Epoch 12/100\n",
      "4760/4760 [==============================] - 706s 148ms/step - loss: 0.0073 - mean_squared_error: 0.0064 - mean_absolute_error: 0.0579 - val_loss: 0.0075 - val_mean_squared_error: 0.0066 - val_mean_absolute_error: 0.0557\n",
      "Epoch 13/100\n",
      "4760/4760 [==============================] - 705s 148ms/step - loss: 0.0071 - mean_squared_error: 0.0063 - mean_absolute_error: 0.0573 - val_loss: 0.0057 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0502\n",
      "Epoch 14/100\n",
      "4760/4760 [==============================] - 725s 152ms/step - loss: 0.0068 - mean_squared_error: 0.0061 - mean_absolute_error: 0.0564 - val_loss: 0.0051 - val_mean_squared_error: 0.0044 - val_mean_absolute_error: 0.0491\n",
      "Epoch 15/100\n",
      "  21/4760 [..............................] - ETA: 12:44 - loss: 0.0061 - mean_squared_error: 0.0053 - mean_absolute_error: 0.0555"
     ]
    }
   ],
   "source": [
    "# Assuming you have already set up your model\n",
    "\n",
    "# Create generators\n",
    "train_generator = memmap_batch_generator(x_memmap_path, y_memmap_path, batch_size, shuffle=True)\n",
    "validation_generator = memmap_batch_generator(x_memmap_path, y_memmap_path, batch_size, shuffle=False)  # Assuming you can use the same for simplicity\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=100,  # Adjust as needed\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "from keras.models import load_model\n",
    "model = load_model('./models/eye_gaze_v23V99.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 7s 13ms/step - loss: 0.0051 - mean_squared_error: 0.0034 - mean_absolute_error: 0.0425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005079503171145916, 0.003443555673584342, 0.04253864660859108]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Evaluate on a test set\n",
    "x_test_memmap = np.memmap('x_dataset.memmap', dtype=x_dtype, mode='r', shape=(num_samples,) + x_shape)\n",
    "y_test_memmap = np.memmap('y_dataset.memmap', dtype=y_dtype, mode='r', shape=(num_samples,) + y_shape)\n",
    "\n",
    "# Assuming the last 15% of the data is for testing\n",
    "test_start_index = int(num_samples * 0.99)\n",
    "x_test = x_test_memmap[test_start_index:]\n",
    "y_test = y_test_memmap[test_start_index:]\n",
    "\n",
    "model.evaluate(x_test, y_test, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/eye_gaze_v23v99.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_9_tf_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
