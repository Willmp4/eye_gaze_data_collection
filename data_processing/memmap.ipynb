{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 217636\n",
    "x_shape = (100, 200, 3)\n",
    "y_shape = (8,)\n",
    "\n",
    "x_dtype = 'float32'  # Determine the appropriate dtype\n",
    "y_dtype = 'float32'  # Determine the appropriate dtype\n",
    "\n",
    "# x_memmap = np.memmap('x_dataset_head_pos.memmap', dtype=x_dtype, mode='w+', shape=(num_samples,) + x_shape)\n",
    "# y_memmap = np.memmap('y_dataset_head_pos.memmap', dtype=y_dtype, mode='w+', shape=(num_samples,) + y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load x_memmap and y_memmap\n",
    "x_memmap = np.memmap('x_dataset_head_pos.memmap', dtype=x_dtype, mode='r+', shape=(num_samples,) + x_shape)\n",
    "y_memmap = np.memmap('y_dataset_head_pos.memmap', dtype=y_dtype, mode='r+', shape=(num_samples,) + y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def process_and_combine_pkl_files_to_memmap(directory_path, x_memmap, y_memmap):\n",
    "    current_index = 0\n",
    "    \n",
    "    for file_path in glob.glob(directory_path + '/*.pkl'):\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            \n",
    "        if isinstance(data, dict) and 'X' in data and 'Y' in data:\n",
    "            X_data = data['X']\n",
    "            Y_data = data['Y']\n",
    "            \n",
    "            # Flatten Y_data\n",
    "            flattened_Y_data = []\n",
    "            for y in Y_data:\n",
    "                # Make sure to handle both cases where y[0] could be a list or a numpy array\n",
    "                gaze_data = np.array(y[0], dtype=np.float32) if isinstance(y[0], list) else y[0].astype(np.float32)\n",
    "                head_pose_data = y[1].astype(np.float32)\n",
    "                flattened_y = np.concatenate([gaze_data, head_pose_data])\n",
    "                flattened_Y_data.append(flattened_y)\n",
    "                \n",
    "            Y_data = np.array(flattened_Y_data, dtype=np.float32)\n",
    "        \n",
    "        else:\n",
    "            Y_numeric = []  # Initialize an empty list to hold the processed Y data\n",
    "            for y in data[1]:\n",
    "                numeric_values = y[:2] + y[3:]  # Adjusted to exclude index 3\n",
    "                Y_numeric.append([float(val) for val in numeric_values])  # Convert to float\n",
    "            \n",
    "            Y_data = np.array(Y_numeric, dtype=np.float32)  # Convert the list to a numpy array of type float32\n",
    "            X_data = np.array(data[0], dtype=np.float32)  # Ensure X_data is also properly formatted\n",
    "\n",
    "        num_samples_in_file = len(X_data)\n",
    "        x_batch = np.array(X_data, dtype=x_memmap.dtype).reshape((num_samples_in_file,) + x_shape)\n",
    "        \n",
    "        # No need to reshape Y_data as it is already in the correct shape after flattening\n",
    "        y_batch = Y_data  # It should already be in the correct shape\n",
    "        \n",
    "        # Ensure we do not exceed the allocated memmap size\n",
    "        if current_index + num_samples_in_file > len(x_memmap):\n",
    "            raise ValueError(\"The dataset is larger than expected.\")\n",
    "        \n",
    "        # Write directly to the memmap files\n",
    "        x_memmap[current_index:current_index + num_samples_in_file] = x_batch\n",
    "        y_memmap[current_index:current_index + num_samples_in_file] = y_batch\n",
    "        \n",
    "        current_index += num_samples_in_file\n",
    "        x_memmap.flush()\n",
    "        y_memmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './process_MPIIGaze/batches_head_pos/' \n",
    "process_and_combine_pkl_files_to_memmap(directory_path, x_memmap, y_memmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memmap_batch_generator(x_memmap_path, y_memmap_path, batch_size, indices, shuffle=True):\n",
    "    x_memmap = np.memmap(x_memmap_path, dtype=x_dtype, mode='r', shape=(num_samples,) + x_shape)\n",
    "    y_memmap = np.memmap(y_memmap_path, dtype=y_dtype, mode='r', shape=(num_samples,) + y_shape)\n",
    "    \n",
    "    if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "    while True:\n",
    "        for start_idx in range(0, len(indices), batch_size):\n",
    "            end_idx = min(start_idx + batch_size, len(indices))\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            # Yield a batch of data\n",
    "            yield x_memmap[batch_indices], y_memmap[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the memmap files\n",
    "x_memmap_path = 'x_dataset_head_pos.memmap'\n",
    "y_memmap_path = 'y_dataset_head_pos.memmap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(100, 200, 3))\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(32, (7, 7), activation='relu', kernel_regularizer=l2(0.001))(input_layer)\n",
    "x = Conv2D(64, (7, 7), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(128, (5, 5), activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.15)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(256, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(512, (3, 3), activation='relu')(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(1024, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.175)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Output layer: Adjusted to output 8 values (2 for gaze, 6 for head pose)\n",
    "output_layer = Dense(8, activation='sigmoid')(x)\n",
    "\n",
    "# Model definition\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00005), loss='mse', metrics=['mean_squared_error', 'mean_absolute_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('eye_gaze_v25_v1.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# EarlyStopping to stop training when the validation loss has not improved after 10 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 372/5780 [>.............................] - ETA: 21:52 - loss: 0.0899 - mean_squared_error: 0.0248 - mean_absolute_error: 0.1100"
     ]
    }
   ],
   "source": [
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices into training and validation sets\n",
    "train_indices = indices[:int(0.85 * num_samples)]  # 85% for training\n",
    "val_indices = indices[int(0.85 * num_samples):]  # 15% for validation\n",
    "\n",
    "# Instantiate the generators\n",
    "train_generator = memmap_batch_generator(x_memmap_path, y_memmap_path, batch_size, train_indices, shuffle=True)\n",
    "validation_generator = memmap_batch_generator(x_memmap_path, y_memmap_path, batch_size, val_indices, shuffle=False)\n",
    "\n",
    "# Calculate steps\n",
    "steps_per_epoch = len(train_indices) // batch_size\n",
    "validation_steps = len(val_indices) // batch_size\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x=train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=100,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Your model definition\n",
    "model = Sequential([\n",
    "    Conv2D(32, (7, 7), activation='relu', input_shape=(100, 200, 3), kernel_regularizer=l2(0.001)),\n",
    "    \n",
    "    \n",
    "    Conv2D(64, (7, 7), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    " \n",
    "\n",
    "    Conv2D(128, (5, 5), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.15),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(512, (3, 3), activation='relu'),\n",
    "    # MaxPooling2D((2, 2)),\n",
    "    Dropout(0.15),\n",
    "\n",
    "    Conv2D(1024, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.175),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer= Adam(learning_rate=0.00005), loss='mse', metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback to save every 5 epochs\n",
    "checkpoint = ModelCheckpoint('eye_gaze_v23_2_{epoch:02d}.h5', save_freq=5*steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already set up your model\n",
    "\n",
    "# Create generators\n",
    "train_generator = memmap_batch_generator(x_memmap_path, y_memmap_path, batch_size, shuffle=True)\n",
    "validation_generator = memmap_batch_generator(x_memmap_path, y_memmap_path, batch_size, shuffle=False)  # Assuming you can use the same for simplicity\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=100,  # Adjust as needed\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "from keras.models import load_model\n",
    "model = load_model('./models/eye_gaze_v23V99.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 7s 13ms/step - loss: 0.0051 - mean_squared_error: 0.0034 - mean_absolute_error: 0.0425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005079503171145916, 0.003443555673584342, 0.04253864660859108]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Evaluate on a test set\n",
    "x_test_memmap = np.memmap('x_dataset.memmap', dtype=x_dtype, mode='r', shape=(num_samples,) + x_shape)\n",
    "y_test_memmap = np.memmap('y_dataset.memmap', dtype=y_dtype, mode='r', shape=(num_samples,) + y_shape)\n",
    "\n",
    "# Assuming the last 15% of the data is for testing\n",
    "test_start_index = int(num_samples * 0.99)\n",
    "x_test = x_test_memmap[test_start_index:]\n",
    "y_test = y_test_memmap[test_start_index:]\n",
    "\n",
    "model.evaluate(x_test, y_test, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/eye_gaze_v23v99.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_9_tf_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
